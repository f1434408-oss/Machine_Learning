# ğŸ“˜ Diplomado en Machine Learning y Deep Learning

---

## ğŸ§¹ Semana 2 â€“ Preprocesamiento de Datos

### ğŸ”¹ Conceptos
- Limpieza de datos (valores nulos, outliers)
- NormalizaciÃ³n y estandarizaciÃ³n
- CodificaciÃ³n de variables categÃ³ricas (One-Hot Encoding, Label Encoding)
- Feature Scaling
- DivisiÃ³n Train / Test

### ğŸ”¹ Importancia
Un buen preprocesamiento puede impactar mÃ¡s en el rendimiento que el modelo en sÃ­.

---

## ğŸ“ˆ Semana 3 â€“ RegresiÃ³n

### ğŸ”¹ Modelos
- RegresiÃ³n Lineal Simple
- RegresiÃ³n Lineal MÃºltiple
- RegularizaciÃ³n (Ridge, Lasso)

### ğŸ”¹ MÃ©tricas
- MAE (Error Absoluto Medio)
- MSE (Error CuadrÃ¡tico Medio)
- RMSE (RaÃ­z del Error CuadrÃ¡tico Medio)
- RÂ² (Coeficiente de determinaciÃ³n)

### ğŸ”¹ Conceptos
- Supuestos del modelo lineal
- Multicolinealidad
- Overfitting

---

## ğŸ¯ Semana 4 â€“ ClasificaciÃ³n

### ğŸ”¹ Modelos
- KNN
- Naive Bayes
- RegresiÃ³n LogÃ­stica

### ğŸ”¹ MÃ©tricas
- Accuracy
- Precision
- Recall
- F1 Score
- Matriz de ConfusiÃ³n
- ROC-AUC

---

## ğŸ§  Semana 5 â€“ SelecciÃ³n de Modelos

### ğŸ”¹ Conceptos
- Bias vs Variance
- Overfitting vs Underfitting
- ValidaciÃ³n Cruzada (K-Fold)
- Grid Search
- Train / Validation / Test Split

---

## ğŸŒ³ Semana 6 â€“ Ãrboles de DecisiÃ³n

### ğŸ”¹ Conceptos
- EntropÃ­a
- Gini
- Information Gain
- Pruning (Poda)

### ğŸ”¹ Ventajas
- Alta interpretabilidad
- No requiere normalizaciÃ³n

### ğŸ”¹ Riesgos
- Sensible al ruido
- Tendencia al overfitting

---

## ğŸ“‰ Semana 7 â€“ ReducciÃ³n de Dimensionalidad

### ğŸ”¹ MÃ©todos
- PCA (Principal Component Analysis)
- Variance Threshold
- EliminaciÃ³n por correlaciÃ³n

### ğŸ”¹ Objetivo
- Reducir multicolinealidad
- Mejorar eficiencia computacional
- Mitigar la "curse of dimensionality"

---

## ğŸ“Š Semana 8 â€“ PresentaciÃ³n Parcial

### ğŸ”¹ IncluyÃ³
- AnÃ¡lisis Exploratorio de Datos (EDA)
- SelecciÃ³n de variables
- JustificaciÃ³n del modelo
- InterpretaciÃ³n de mÃ©tricas

---

## âš™ï¸ Semana 9 â€“ MÃ¡quinas de Vectores de Soporte (SVM)

### ğŸ”¹ Conceptos
- Hiperplano Ã³ptimo
- Margen mÃ¡ximo
- Kernel Trick

### ğŸ”¹ Tipos de Kernel
- Lineal
- Polinomial
- RBF

---

## ğŸŒ² Semana 10 â€“ Modelos de Ensamble

### ğŸ”¹ Modelos
- Random Forest
- Gradient Boosting
- XGBoost

### ğŸ”¹ Idea Clave
Combinar mÃºltiples modelos dÃ©biles para construir un modelo mÃ¡s robusto y preciso.

---

## ğŸ“ Semana 11 â€“ Clustering

### ğŸ”¹ MÃ©todos
- K-Means
- Clustering JerÃ¡rquico
- DBSCAN

### ğŸ”¹ EvaluaciÃ³n
- Silhouette Score
- MÃ©todo del Codo (Elbow Method)

---

## ğŸ“ˆ Semana 12 â€“ RegresiÃ³n LogÃ­stica

### ğŸ”¹ Conceptos
- FunciÃ³n Sigmoide
- Log-Loss
- Odds Ratio

### ğŸ”¹ AplicaciÃ³n
- ClasificaciÃ³n binaria
- Probabilidad de pertenencia a clase

---

## ğŸ§  Semana 13 â€“ Redes Neuronales

### ğŸ”¹ Conceptos
- PerceptrÃ³n
- Funciones de activaciÃ³n (ReLU, Sigmoid, Tanh)
- Backpropagation
- Epochs y Batch Size

### ğŸ”¹ Arquitectura bÃ¡sica
- Capa de entrada
- Capas ocultas
- Capa de salida

---

## ğŸ–¼ Semana 14 â€“ Redes Convolucionales (CNN)

### ğŸ”¹ Conceptos
- Filtros
- ConvoluciÃ³n
- Pooling
- Flatten
- Capas densas

### ğŸ”¹ AplicaciÃ³n
- VisiÃ³n por computadora
- ClasificaciÃ³n de imÃ¡genes

---

## ğŸ” Semana 15 â€“ Transfer Learning

### ğŸ”¹ Conceptos
- Modelos preentrenados
- Fine-tuning
- Congelamiento de capas

### ğŸ”¹ Ventaja
Reduce tiempo de entrenamiento y mejora desempeÃ±o cuando se dispone de pocos datos.

---

# ğŸ¯ ConclusiÃ³n

Este diplomado reforzÃ³ conocimientos en:

- Modelos supervisados y no supervisados
- TÃ©cnicas de ensamble
- Deep Learning
- Computer Vision
- EvaluaciÃ³n y selecciÃ³n de modelos
- Pipeline completo de Machine Learning
